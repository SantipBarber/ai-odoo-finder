# Fase 1: Diagn√≥stico y Benchmark - Especificaciones T√©cnicas

**Fecha:** 22 Noviembre 2025
**Proyecto:** AI-OdooFinder
**Fase:** 1 - Diagn√≥stico y Benchmark
**Duraci√≥n Estimada:** 1 d√≠a
**Prioridad:** Alta (Bloqueante para fases siguientes)

---

## üìã Objetivo

Establecer un sistema de evaluaci√≥n cuantitativa de la calidad de b√∫squeda actual mediante:

1. **Benchmark Suite**: 20 b√∫squedas representativas con resultados esperados documentados
2. **M√©tricas Baseline**: Precisi√≥n, recall y MRR del sistema actual
3. **An√°lisis de Patrones**: Identificar tipos de b√∫squedas que fallan sistem√°ticamente

---

## üéØ Entregables

| # | Entregable | Archivo | Criterio de √âxito |
|---|------------|---------|-------------------|
| 1 | Suite de queries de benchmark | `tests/benchmark_queries.json` | 20 queries validadas, 5 categor√≠as cubiertas |
| 2 | Script de benchmark | `scripts/run_benchmark.py` | Ejecuta y calcula 4 m√©tricas autom√°ticamente |
| 3 | Resultado baseline | `tests/results/baseline_YYYYMMDD.json` | Precision@3 < 40% (confirma necesidad mejoras) |
| 4 | An√°lisis de fallos | `tests/results/failure_analysis.md` | 5 patrones documentados con ejemplos |

---

## üìö Especificaciones

1. [SPEC-001: Benchmark Queries Dataset](./SPEC-001-benchmark-queries.md)
2. [SPEC-002: Benchmark Execution Script](./SPEC-002-benchmark-script.md)
3. [SPEC-003: Metrics Calculation](./SPEC-003-metrics.md)
4. [SPEC-004: Acceptance Criteria](./SPEC-004-acceptance-criteria.md)

---

## üîÑ Flujo de Trabajo

```mermaid
graph TD
    A[Crear benchmark_queries.json] --> B[Implementar run_benchmark.py]
    B --> C[Ejecutar baseline]
    C --> D[Analizar resultados]
    D --> E{Precision@3 < 40%?}
    E -->|S√≠| F[‚úÖ Fase 1 Completada]
    E -->|No| G[‚ö†Ô∏è Revisar queries o sistema]
    F --> H[Documentar patrones de fallo]
    H --> I[Iniciar Fase 2]
```

---

## üß™ Tests de Validaci√≥n

### Test 1: Benchmark Queries V√°lidas
```bash
# Validar que todas las queries son ejecutables
python -m pytest tests/test_benchmark_queries.py::test_all_queries_are_valid
```

**Criterio:** Todas las 20 queries deben tener:
- Campo `query` no vac√≠o
- Campo `version` v√°lido (12.0-19.0)
- Al menos 1 m√≥dulo esperado

### Test 2: Script Ejecutable
```bash
# El script debe completarse sin errores
python scripts/run_benchmark.py
```

**Criterio:**
- Sin excepciones
- Genera archivo JSON en `tests/results/`
- Output muestra progreso

### Test 3: M√©tricas Calculadas
```bash
# Validar que las m√©tricas est√°n presentes
python -m pytest tests/test_benchmark_metrics.py::test_metrics_present
```

**Criterio:** El resultado contiene:
- `aggregate_metrics.precision@3`
- `aggregate_metrics.precision@5`
- `aggregate_metrics.recall@10`
- `aggregate_metrics.mrr`

---

## üìä M√©tricas de √âxito (Fase 1)

| M√©trica | Target | Justificaci√≥n |
|---------|--------|---------------|
| Precision@3 | < 40% | Confirma que hay margen de mejora significativo |
| Recall@10 | Cualquier valor | Baseline para comparaci√≥n futura |
| MRR | Cualquier valor | Baseline para comparaci√≥n futura |
| Queries ejecutadas | 20/20 | 100% de cobertura del benchmark |
| Tiempo ejecuci√≥n | < 5 min | Feedback r√°pido para iteraci√≥n |

---

## üö® Riesgos y Mitigaciones

### Riesgo 1: Queries mal dise√±adas (esperados incorrectos)
**Impacto:** Alto - Invalidar√≠a todo el benchmark
**Probabilidad:** Media
**Mitigaci√≥n:**
- Review manual de queries cr√≠ticas
- Validar al menos 5 queries manualmente antes de ejecutar
- Incluir queries de diferentes dificultades

### Riesgo 2: Sistema actual ya funciona bien (Precision > 60%)
**Impacto:** Bajo - Ser√≠a buena noticia
**Probabilidad:** Baja (seg√∫n contexto del proyecto)
**Mitigaci√≥n:**
- Si sucede, ajustar targets de mejora
- Enfocar en casos edge m√°s dif√≠ciles

### Riesgo 3: API de embeddings falla durante benchmark
**Impacto:** Medio - Retrasar√≠a la fase
**Probabilidad:** Baja
**Mitigaci√≥n:**
- Implementar retry logic con backoff
- Cache de embeddings de queries

---

## üîß Dependencias T√©cnicas

### Nuevas Dependencias (si aplica)
Ninguna - Usa stack existente.

### Servicios Externos
- **OpenRouter API**: Para generar embeddings de queries
- **PostgreSQL**: Base de datos con m√≥dulos indexados

### Archivos Modificados
Ninguno - Solo crea nuevos archivos.

---

## üìù Notas de Implementaci√≥n

### Orden de Implementaci√≥n Recomendado

1. **Primero:** Crear `benchmark_queries.json` (manual, revisar con cuidado)
2. **Segundo:** Implementar funciones de m√©tricas (unit testeable)
3. **Tercero:** Implementar script de benchmark (orquestador)
4. **Cuarto:** Ejecutar y analizar resultados
5. **Quinto:** Documentar patrones de fallo

### Consideraciones Especiales

- **Queries en espa√±ol:** Asegurar que el modelo de embeddings maneja bien espa√±ol
- **Versiones Odoo:** Cubrir al menos 3 versiones diferentes (e.g., 16.0, 17.0, 18.0)
- **Categor√≠as balanceadas:** No todas las queries de la misma √°rea funcional

---

## ‚úÖ Checklist de Implementaci√≥n

- [ ] Crear estructura de directorios `tests/benchmark_queries.json` y `tests/results/`
- [ ] Implementar `tests/benchmark_queries.json` con 20 queries validadas
- [ ] Implementar `scripts/run_benchmark.py`
- [ ] Implementar funciones de c√°lculo de m√©tricas
- [ ] Ejecutar benchmark y generar `baseline_YYYYMMDD.json`
- [ ] Analizar resultados y crear `failure_analysis.md`
- [ ] Validar que Precision@3 < 40% (o documentar si es mayor)
- [ ] Documentar 5 patrones de fallo identificados
- [ ] Code review de las queries (al menos 2 personas)
- [ ] Commit y push a repositorio

---

## üîó Referencias

- Documento maestro: [docs/SYSTEM_IMPROVEMENTS.md](../../docs/SYSTEM_IMPROVEMENTS.md) - Secci√≥n "Fase 1"
- M√©tricas IR: [Information Retrieval Metrics](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval))
- Stack actual: FastAPI + PostgreSQL + pgVector + OpenRouter

---

**Estado:** üî¥ Pendiente
**Pr√≥ximo paso:** Implementar SPEC-001 (Benchmark Queries)
