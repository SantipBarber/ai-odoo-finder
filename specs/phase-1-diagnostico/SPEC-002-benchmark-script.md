# SPEC-002: Benchmark Execution Script

**ID:** SPEC-002
**Componente:** Benchmark Runner
**Archivo:** `scripts/run_benchmark.py`
**Prioridad:** Alta
**Estimaci√≥n:** 3-4 horas
**Dependencias:** SPEC-001 (Benchmark Queries)

---

## üìã Descripci√≥n

Implementar un script Python que ejecute todas las queries del benchmark, calcule m√©tricas de informaci√≥n retrieval (IR), y genere un reporte estructurado en JSON con resultados detallados y agregados.

---

## üéØ Objetivos

1. **Automatizaci√≥n:** Ejecutar las 20 queries sin intervenci√≥n manual
2. **M√©tricas est√°ndar:** Calcular Precision@k, Recall@k, MRR
3. **Reporte estructurado:** JSON con resultados detallados y agregados
4. **Trazabilidad:** Timestamp y versionado de resultados
5. **Observabilidad:** Output progresivo durante ejecuci√≥n

---

## üèóÔ∏è Arquitectura

```
run_benchmark.py
‚îú‚îÄ‚îÄ main()                           # Entry point
‚îú‚îÄ‚îÄ BenchmarkRunner
‚îÇ   ‚îú‚îÄ‚îÄ __init__(db_session)
‚îÇ   ‚îú‚îÄ‚îÄ run()                        # Orquestador principal
‚îÇ   ‚îú‚îÄ‚îÄ _load_queries()              # Carga benchmark_queries.json
‚îÇ   ‚îú‚îÄ‚îÄ _execute_query()             # Ejecuta 1 query
‚îÇ   ‚îî‚îÄ‚îÄ _save_results()              # Persiste resultados
‚îú‚îÄ‚îÄ MetricsCalculator (SPEC-003)
‚îÇ   ‚îú‚îÄ‚îÄ precision_at_k()
‚îÇ   ‚îú‚îÄ‚îÄ recall_at_k()
‚îÇ   ‚îú‚îÄ‚îÄ mrr()
‚îÇ   ‚îî‚îÄ‚îÄ calculate_all()
‚îî‚îÄ‚îÄ ReportGenerator
    ‚îú‚îÄ‚îÄ aggregate_metrics()
    ‚îú‚îÄ‚îÄ per_category_metrics()
    ‚îú‚îÄ‚îÄ per_difficulty_metrics()
    ‚îî‚îÄ‚îÄ format_output()
```

---

## üìê Interfaz y Firma de Funciones

### Clase Principal: BenchmarkRunner

```python
from typing import List, Dict, Optional
from sqlalchemy.ext.asyncio import AsyncSession
from datetime import datetime
from app.services.search_service import SearchService

class BenchmarkRunner:
    """Ejecuta benchmark de b√∫squeda y genera reporte."""

    def __init__(self, db: AsyncSession):
        """
        Inicializa el runner.

        Args:
            db: Sesi√≥n async de base de datos
        """
        self.db = db
        self.search_service = SearchService(db)
        self.metrics_calculator = MetricsCalculator()
        self.report_generator = ReportGenerator()

    async def run(
        self,
        output_dir: str = "tests/results",
        limit: int = 10,
        verbose: bool = True
    ) -> Dict:
        """
        Ejecuta el benchmark completo.

        Args:
            output_dir: Directorio donde guardar resultados
            limit: N√∫mero de resultados a retornar por query (para calcular recall@10)
            verbose: Si True, imprime progreso

        Returns:
            Dict con resultados completos del benchmark

        Raises:
            FileNotFoundError: Si benchmark_queries.json no existe
            ValueError: Si queries est√°n mal formadas
        """
        ...

    def _load_queries(self, filepath: str = "tests/benchmark_queries.json") -> List[Dict]:
        """
        Carga queries desde JSON.

        Returns:
            Lista de queries validadas

        Raises:
            FileNotFoundError: Si archivo no existe
            json.JSONDecodeError: Si JSON inv√°lido
        """
        ...

    async def _execute_query(self, query_data: Dict, limit: int) -> Dict:
        """
        Ejecuta una query individual.

        Args:
            query_data: Objeto query del benchmark
            limit: N√∫mero de resultados a retornar

        Returns:
            Dict con:
                - query_id
                - query
                - returned_modules (lista de technical_names)
                - execution_time_ms
                - error (si hubo)
        """
        ...

    def _save_results(
        self,
        results: Dict,
        output_dir: str
    ) -> str:
        """
        Guarda resultados en archivo JSON timestamped.

        Args:
            results: Resultados completos
            output_dir: Directorio destino

        Returns:
            Path del archivo creado
        """
        ...
```

---

## üìä Estructura del Output

### Archivo: `tests/results/baseline_YYYYMMDD_HHMMSS.json`

```json
{
  "metadata": {
    "timestamp": "2025-11-22T10:30:45Z",
    "total_queries": 20,
    "search_mode": "vector",
    "limit": 10,
    "execution_time_seconds": 45.2
  },
  "aggregate_metrics": {
    "precision@3": 0.35,
    "precision@5": 0.42,
    "recall@10": 0.58,
    "mrr": 0.412
  },
  "per_category": {
    "sales_workflow": {
      "count": 3,
      "precision@3": 0.33,
      "precision@5": 0.40
    },
    "accounting": {
      "count": 3,
      "precision@3": 0.44,
      "precision@5": 0.50
    }
    // ... m√°s categor√≠as
  },
  "per_difficulty": {
    "easy": {
      "count": 5,
      "precision@3": 0.60,
      "precision@5": 0.68
    },
    "medium": {
      "count": 10,
      "precision@3": 0.30,
      "precision@5": 0.38
    },
    "hard": {
      "count": 5,
      "precision@3": 0.13,
      "precision@5": 0.20
    }
  },
  "detailed_results": [
    {
      "query_id": 1,
      "query": "facturaci√≥n electr√≥nica Espa√±a",
      "version": "16.0",
      "category": "localization_spain",
      "difficulty": "easy",
      "expected_modules": ["l10n_es_facturae", "l10n_es_aeat"],
      "returned_modules": [
        "l10n_es_facturae",      // ‚úÖ Match
        "l10n_es_aeat",          // ‚úÖ Match
        "l10n_es_vat_book",
        "account_invoice",
        "l10n_es"
      ],
      "metrics": {
        "precision@3": 0.667,   // 2/3 relevantes
        "precision@5": 0.40,    // 2/5 relevantes
        "recall@10": 1.0,       // 2/2 esperados encontrados
        "mrr": 1.0,             // Primer resultado es relevante
        "hits_in_top_3": 2,
        "hits_in_top_5": 2,
        "first_relevant_position": 1
      },
      "execution_time_ms": 234
    }
    // ... 19 queries m√°s
  ]
}
```

---

## üîç L√≥gica de C√°lculo de M√©tricas

### Precision@k

```python
def calculate_precision_at_k(
    retrieved: List[str],
    expected: List[str],
    k: int
) -> float:
    """
    Calcula precision@k: fracci√≥n de resultados relevantes en top K.

    Formula: P@k = (# relevantes en top K) / K

    Args:
        retrieved: Lista de m√≥dulos retornados (en orden)
        expected: Lista de m√≥dulos esperados (ground truth)
        k: Cutoff (t√≠picamente 3 o 5)

    Returns:
        Precision en [0, 1]

    Example:
        >>> calculate_precision_at_k(
        ...     retrieved=["mod1", "mod2", "mod3", "mod4"],
        ...     expected=["mod1", "mod3"],
        ...     k=3
        ... )
        0.667  # 2 de 3 son relevantes
    """
    if not retrieved or k == 0:
        return 0.0

    top_k = retrieved[:k]
    relevant_count = sum(1 for mod in top_k if mod in expected)

    return relevant_count / k
```

### Recall@k

```python
def calculate_recall_at_k(
    retrieved: List[str],
    expected: List[str],
    k: int
) -> float:
    """
    Calcula recall@k: fracci√≥n de esperados que est√°n en top K.

    Formula: R@k = (# esperados en top K) / (# total esperados)

    Args:
        retrieved: Lista de m√≥dulos retornados
        expected: Lista de m√≥dulos esperados
        k: Cutoff

    Returns:
        Recall en [0, 1]

    Example:
        >>> calculate_recall_at_k(
        ...     retrieved=["mod1", "mod2", "mod3"],
        ...     expected=["mod1", "mod3", "mod5"],
        ...     k=3
        ... )
        0.667  # 2 de 3 esperados fueron encontrados
    """
    if not expected:
        return 0.0

    top_k = retrieved[:k]
    found_count = sum(1 for exp in expected if exp in top_k)

    return found_count / len(expected)
```

### Mean Reciprocal Rank (MRR)

```python
def calculate_mrr(
    retrieved: List[str],
    expected: List[str]
) -> float:
    """
    Calcula Mean Reciprocal Rank: inverso del rank del primer relevante.

    Formula: MRR = 1 / (posici√≥n primer relevante)

    Args:
        retrieved: Lista de m√≥dulos retornados (ordenados)
        expected: Lista de m√≥dulos esperados

    Returns:
        MRR en [0, 1]

    Example:
        >>> calculate_mrr(
        ...     retrieved=["mod1", "mod2", "mod3", "mod4"],
        ...     expected=["mod3", "mod5"]
        ... )
        0.333  # Primer relevante en posici√≥n 3: 1/3
    """
    for i, module in enumerate(retrieved, start=1):
        if module in expected:
            return 1.0 / i

    return 0.0  # Ning√∫n relevante encontrado
```

---

## üñ•Ô∏è Output de Consola

### Durante Ejecuci√≥n (Verbose Mode)

```
================================================================================
AI-OdooFinder Benchmark Runner
================================================================================

Loading queries from: tests/benchmark_queries.json
‚úì Loaded 20 queries successfully

Starting benchmark execution...

[1/20] Query: "facturaci√≥n electr√≥nica Espa√±a"
       Version: 16.0 | Category: localization_spain | Difficulty: easy
       Expected: 2 modules
       ‚úì Executed in 234ms
       Metrics: P@3=0.667 | P@5=0.40 | R@10=1.0 | MRR=1.0

[2/20] Query: "separar flujos B2B y B2C"
       Version: 16.0 | Category: sales_workflow | Difficulty: medium
       Expected: 3 modules
       ‚úì Executed in 189ms
       Metrics: P@3=0.333 | P@5=0.40 | R@10=0.667 | MRR=0.5

...

[20/20] Query: "cross-docking con proveedores"
        Version: 18.0 | Category: inventory | Difficulty: hard
        Expected: 3 modules
        ‚úì Executed in 201ms
        Metrics: P@3=0.0 | P@5=0.20 | R@10=0.333 | MRR=0.0

================================================================================
BENCHMARK COMPLETED
================================================================================

Execution Time: 45.2 seconds
Total Queries: 20

AGGREGATE METRICS:
  Precision@3:  35.0%  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
  Precision@5:  42.0%  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
  Recall@10:    58.0%  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
  Mean MRR:     0.412

BY DIFFICULTY:
  Easy    (5):  P@3=60.0% | P@5=68.0%
  Medium (10):  P@3=30.0% | P@5=38.0%
  Hard    (5):  P@3=13.0% | P@5=20.0%

Results saved to: tests/results/baseline_20251122_103045.json

================================================================================
```

---

## ‚úÖ Criterios de Aceptaci√≥n

### Criterio 1: Ejecuci√≥n Completa
- ‚úÖ Ejecuta las 20 queries sin fallos
- ‚úÖ Maneja errores gracefully (timeout, API errors)
- ‚úÖ No crashea si 1-2 queries fallan

### Criterio 2: M√©tricas Correctas
- ‚úÖ Calcula Precision@3, Precision@5, Recall@10, MRR
- ‚úÖ M√©tricas agregadas correctamente
- ‚úÖ M√©tricas por categor√≠a y dificultad

### Criterio 3: Output
- ‚úÖ Genera archivo JSON v√°lido
- ‚úÖ Filename con timestamp: `baseline_YYYYMMDD_HHMMSS.json`
- ‚úÖ Output de consola informativo

### Criterio 4: Performance
- ‚úÖ Completa en < 5 minutos (20 queries √ó ~10s cada una)
- ‚úÖ No hace queries innecesarias a la BD

---

## üß™ Tests Unitarios

### Test 1: Metrics Calculation

```python
# tests/test_metrics.py

def test_precision_at_k():
    """Test precision calculation."""

    retrieved = ["mod1", "mod2", "mod3", "mod4"]
    expected = ["mod1", "mod3", "mod5"]

    # Top 3: 2 relevantes (mod1, mod3)
    assert calculate_precision_at_k(retrieved, expected, k=3) == 2/3

    # Top 5: 2 relevantes
    assert calculate_precision_at_k(retrieved, expected, k=5) == 0.4


def test_recall_at_k():
    """Test recall calculation."""

    retrieved = ["mod1", "mod2", "mod3"]
    expected = ["mod1", "mod3", "mod5"]

    # Found 2 out of 3 expected
    assert calculate_recall_at_k(retrieved, expected, k=3) == 2/3


def test_mrr():
    """Test MRR calculation."""

    # First relevant at position 1
    assert calculate_mrr(["mod1", "mod2"], ["mod1"]) == 1.0

    # First relevant at position 3
    assert calculate_mrr(["mod1", "mod2", "mod3"], ["mod3"]) == 1/3

    # No relevant
    assert calculate_mrr(["mod1", "mod2"], ["mod3"]) == 0.0
```

### Test 2: Query Loading

```python
@pytest.mark.asyncio
async def test_load_queries():
    """Test loading benchmark queries."""

    runner = BenchmarkRunner(db_session)
    queries = runner._load_queries("tests/benchmark_queries.json")

    assert len(queries) == 20
    assert all('query' in q for q in queries)
    assert all('expected_modules' in q for q in queries)
```

### Test 3: End-to-End (Sample)

```python
@pytest.mark.asyncio
async def test_benchmark_execution_sample(db_session):
    """Test executing a single query."""

    runner = BenchmarkRunner(db_session)

    query_data = {
        "id": 1,
        "query": "facturaci√≥n electr√≥nica Espa√±a",
        "version": "16.0",
        "expected_modules": ["l10n_es_facturae", "l10n_es_aeat"],
        "category": "localization_spain",
        "difficulty": "easy"
    }

    result = await runner._execute_query(query_data, limit=10)

    assert 'query_id' in result
    assert 'returned_modules' in result
    assert 'metrics' in result
    assert len(result['returned_modules']) <= 10
```

---

## üö® Manejo de Errores

### Error 1: API Timeout
```python
try:
    results = await search_service.search_modules(...)
except asyncio.TimeoutError:
    logger.warning(f"Query {query_id} timed out")
    # Return partial result with error flag
    return {
        "query_id": query_id,
        "error": "timeout",
        "returned_modules": [],
        "metrics": None
    }
```

### Error 2: M√≥dulo Esperado No Existe
```python
# Validar durante carga de queries
for expected_module in query['expected_modules']:
    exists = await db.execute(
        text("SELECT 1 FROM odoo_modules WHERE technical_name = :name"),
        {"name": expected_module}
    )
    if not exists.scalar():
        logger.error(f"Expected module '{expected_module}' not found in DB")
        # Opcional: Skip query o marcar como inv√°lida
```

### Error 3: Archivo JSON Corrupto
```python
try:
    with open(filepath, 'r') as f:
        data = json.load(f)
except json.JSONDecodeError as e:
    logger.error(f"Invalid JSON in {filepath}: {e}")
    raise ValueError(f"Benchmark queries file is corrupted: {e}")
```

---

## üöÄ Pasos de Implementaci√≥n

1. **Crear estructura base**
   - Archivo `scripts/run_benchmark.py`
   - Imports necesarios

2. **Implementar MetricsCalculator** (ver SPEC-003)
   - `precision_at_k()`
   - `recall_at_k()`
   - `mrr()`

3. **Implementar BenchmarkRunner**
   - `_load_queries()`
   - `_execute_query()`
   - `_save_results()`

4. **Implementar ReportGenerator**
   - `aggregate_metrics()`
   - `per_category_metrics()`
   - `per_difficulty_metrics()`

5. **Implementar main()**
   - CLI interface
   - Orquestaci√≥n

6. **A√±adir logging y progress**
   - Console output
   - Progress bar (opcional con `tqdm`)

7. **Testing**
   - Unit tests para m√©tricas
   - Integration test con queries de ejemplo

---

## üîó Siguiente Paso

Una vez completado este SPEC, proceder a:
‚Üí [SPEC-003: Metrics Calculation](./SPEC-003-metrics.md)

---

**Estado:** üî¥ Pendiente
**Implementador:** TBD
**Revisor:** TBD
